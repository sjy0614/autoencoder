import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.callbacks import EarlyStopping

# -----------------------------------
# ğŸ§­ ê¸°ë³¸ ì„¤ì •
# -----------------------------------
st.set_page_config(
    page_title="ì œì¡° ì„¤ë¹„ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ",
    layout="wide",
    page_icon="ğŸ­"
)

st.title("ğŸ­ ì œì¡° ì„¤ë¹„ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ")
st.markdown("""
ì—…ë¡œë“œëœ **ì„¼ì„œ ë¡œê·¸ ë°ì´í„°(CSV)**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ  
ìë™ìœ¼ë¡œ ì´ìƒì¹˜ êµ¬ê°„ê³¼ ë°œìƒ ì‹œì ì„ ë¶„ì„í•©ë‹ˆë‹¤.
""")

# -----------------------------------
# ğŸ“‚ íŒŒì¼ ì—…ë¡œë“œ
# -----------------------------------
uploaded_file = st.file_uploader("ğŸ“ ì„¼ì„œ ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ (CSV í˜•ì‹)", type=["csv"])

if uploaded_file is not None:
    df = pd.read_csv(uploaded_file)
    st.success("âœ… ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ!")

    # -----------------------------------
    # ğŸ§¹ ë°ì´í„° ì •ì œ
    # -----------------------------------
    if 'sensor_15' in df.columns:
        df = df.drop(columns=['sensor_15'])
    df = df.drop(columns=['Unnamed: 0'], errors='ignore')
    df.columns = df.columns.str.strip().str.lower()

    if 'timestamp' not in df.columns:
        st.error("âŒ 'timestamp' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‹œê°„ ì •ë³´ë¥¼ í¬í•¨í•œ CSVë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.stop()

    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
    df = df.sort_values('timestamp')

    # machine_status ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ optional
    if 'machine_status' in df.columns:
        df['machine_status'] = df['machine_status'].astype(str).str.strip().str.upper()
    else:
        df['machine_status'] = "UNKNOWN"

    # -----------------------------------
    # ğŸ” ì„¼ì„œ ë°ì´í„°ë§Œ ì¶”ì¶œ
    # -----------------------------------
    sensor_cols = [c for c in df.columns if 'sensor' in c]
    if not sensor_cols:
        st.error("âŒ 'sensor_'ë¡œ ì‹œì‘í•˜ëŠ” ì„¼ì„œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    normal_df = df[df['machine_status'] == 'NORMAL'] if 'NORMAL' in df['machine_status'].unique() else df.copy()
    X_train = normal_df[sensor_cols].apply(pd.to_numeric, errors='coerce')
    X_test = df[sensor_cols].apply(pd.to_numeric, errors='coerce')

    # ê²°ì¸¡ ì²˜ë¦¬
    X_train = X_train.interpolate(limit_direction='both').fillna(X_train.mean())
    X_test = X_test.interpolate(limit_direction='both').fillna(X_train.mean())

    # í‘œì¤€í™”
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # -----------------------------------
    # ğŸ§  Autoencoder ëª¨ë¸
    # -----------------------------------
    input_dim = X_train_scaled.shape[1]
    input_layer = Input(shape=(input_dim,))
    encoded = Dense(32, activation='relu')(input_layer)
    encoded = Dense(16, activation='relu')(encoded)
    bottleneck = Dense(8, activation='relu')(encoded)
    decoded = Dense(16, activation='relu')(bottleneck)
    decoded = Dense(32, activation='relu')(decoded)
    output_layer = Dense(input_dim, activation='linear')(decoded)

    autoencoder = Model(inputs=input_layer, outputs=output_layer)
    autoencoder.compile(optimizer='adam', loss='mse')

    es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    with st.spinner('ëª¨ë¸ í•™ìŠµ ì¤‘... â³'):
        autoencoder.fit(
            X_train_scaled, X_train_scaled,
            epochs=30,
            batch_size=256,
            validation_split=0.2,
            callbacks=[es],
            verbose=0
        )

    st.success("âœ… ì´ìƒ íƒì§€ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

    # -----------------------------------
    # ğŸ“‰ ì´ìƒ íƒì§€
    # -----------------------------------
    reconstructions = autoencoder.predict(X_test_scaled)
    mse = np.mean(np.square(X_test_scaled - reconstructions), axis=1)
    df['reconstruction_error'] = mse

    threshold = np.percentile(mse, 99)
    df['anomaly'] = df['reconstruction_error'] > threshold

    # -----------------------------------
    # ğŸ“Š í†µê³„ ë¶„ì„
    # -----------------------------------
    total_records = len(df)
    num_anomalies = df['anomaly'].sum()
    anomaly_ratio = (num_anomalies / total_records) * 100

    st.subheader("ğŸ“ˆ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    c1, c2, c3 = st.columns(3)
    c1.metric("ì´ ë°ì´í„° ìˆ˜", f"{total_records:,}")
    c2.metric("ì´ìƒì¹˜ íƒì§€ ìˆ˜", f"{num_anomalies:,}")
    c3.metric("ì´ìƒì¹˜ ë¹„ìœ¨", f"{anomaly_ratio:.2f}%")

    if 'machine_status' in df.columns:
        status_summary = df.groupby('machine_status')['anomaly'].agg(['sum', 'mean']).rename(columns={'sum': 'ì´ìƒì¹˜ ìˆ˜', 'mean': 'ë¹„ìœ¨'})
        status_summary['ë¹„ìœ¨'] = (status_summary['ë¹„ìœ¨'] * 100).round(2)
        st.write("### ìƒíƒœë³„ ì´ìƒ íƒì§€ ë¹„ìœ¨")
        st.dataframe(status_summary)

    # -----------------------------------
    # ğŸ“ˆ ì‹œê°í™”
    # -----------------------------------
    st.subheader("ğŸ“Š ì´ìƒì¹˜ ì‹œê°í™”")
    fig, ax = plt.subplots(figsize=(14,6))
    ax.plot(df['timestamp'], df['reconstruction_error'], label='Reconstruction Error', color='steelblue')
    ax.axhline(y=threshold, color='red', linestyle='--', label='Threshold (99%)')
    anomalies = df[df['anomaly']]
    ax.scatter(anomalies['timestamp'], anomalies['reconstruction_error'], color='orange', s=12, label='Anomaly')
    ax.legend()
    ax.set_title("Reconstruction Error Over Time")
    ax.set_xlabel("Timestamp")
    ax.set_ylabel("Reconstruction Error")
    st.pyplot(fig)

    # -----------------------------------
    # ğŸ“ ì´ìƒ ë°œìƒ êµ¬ê°„ ìš”ì•½
    # -----------------------------------
    st.subheader("ğŸ§­ ì´ìƒ ë°œìƒ êµ¬ê°„ ìš”ì•½")
    anomalies_sorted = anomalies[['timestamp', 'reconstruction_error']].sort_values('timestamp')
    if not anomalies_sorted.empty:
        start_time = anomalies_sorted['timestamp'].min()
        end_time = anomalies_sorted['timestamp'].max()
        st.write(f"ğŸ“… **ì´ìƒì¹˜ ë°œìƒ ê¸°ê°„:** {start_time.strftime('%Y-%m-%d %H:%M:%S')} â†’ {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        st.write(f"âš ï¸ **ì´ìƒì¹˜ êµ¬ê°„ ê°œìˆ˜:** {len(anomalies_sorted)}")
        st.dataframe(anomalies_sorted.head(20))
    else:
        st.info("âœ… ì´ìƒì¹˜ê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ëª¨ë“  êµ¬ê°„ ì •ìƒ)")

    # -----------------------------------
    # â¬‡ï¸ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
    # -----------------------------------
    st.download_button(
        label="ğŸ“¥ ì´ìƒ íƒì§€ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
        data=df.to_csv(index=False).encode('utf-8-sig'),
        file_name="anomaly_detection_result.csv",
        mime="text/csv"
    )

else:
    st.info("ğŸ‘† CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ì´ìƒ íƒì§€ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
